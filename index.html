<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Matthew Perez</title>
  
  <meta name="author" content="Matthew Perez">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Matthew Perez</name>
              </p>
              <p>
              My name is Matthew Perez and I am a Machine Learning Research Scientist at <a href="https://teachfx.com/">TeachFX</a>. I graduated from the University of Michigan with a Ph.D. in Computer Science and Engineering working with Dr. Emily Mower Provost at the <a href="https://web.eecs.umich.edu/~emilykmp/chai/">Computational Human Artificial Intelligence (CHAI) Lab</a>. My research interest is broadly in speech- and language-based machine learning applications. My work has spanned topics including speaker diarization, ASR, error characterization, grapheme-to-phoneme, and emotion recognition.
              </p>
              <p>
                I received my M.S. in Computer Science and Engineering at the University of Michigan and my B.S. in Computer Science at the University of Notre Dame.  I have  been fortunate to receive the GEM Fellowship ('19) and the NSF Graduate Research Fellowship ('20).
              </p>

              <p id="lastModified" style="font-size: small; text-align: left;"></p>

            </td>
            
            <td style="padding:2.5%;width:40%;max-width:40%">
              <p style="text-align:center">
              <a href="images/casual.jpeg">
                <!-- <img style="width:100%;max-width:100%" alt="profile photo" src="images/matt.jpeg" class="hoverZoomLink"> -->
                <!-- <img height="150" width="200" alt="profile photo" src="images/matt.jpeg"> -->
                <img height="150" width="200" alt="profile photo" src="images/casual.jpeg">
              </a>
              </p>

              <p style="text-align:center">
              <a href="mailto:mkperez@umich.edu">Email</a> |
              <a href="data/Perez_Resume.pdf">Resume</a> |
              <a href="https://scholar.google.com/citations?user=HRLbzeYAAAAJ">Google Scholar</a> |
              <a href="https://github.com/matthewkperez/">Github</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        



				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf"><papertitle>PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text</papertitle></a>
              <br>
              Yang Yu,
              <strong>Matthew Perez</strong>,
              Anker Bapna,
              Fadi Haik,
              Siamak Tazari,
              Yu Zhang
              <br>
              <em>INTERSPEECH</em>, 2023
              <br>
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf">Paper</a>
              <p></p>
              <p>We present PronScribe, a novel method for phonemic transcription from speech and text input based on careful finetuning and adaptation of a massive, multilingual, multimodal speech-text pretrained model. We show that our model is capable of phonemically transcribing pronunciations of full utterances with accurate word boundaries in a variety of languages covering diverse phonological phenomena, achieving phoneme error rates in the vicinity of 1-2% which is comparable to human transcribers.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/tavernor23_interspeech.pdf"><papertitle>Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition</papertitle></a>
              <br>
              James Tavernor,
              <strong>Matthew Perez</strong>,
              Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2023
              <br>
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/tavernor23_interspeech.pdf">Paper</a>
              <p></p>
              <p>In this paper, we investigate how a model can be adapted to unseen environments without forgetting previously learned environments. We show that memory-based methods maintain performance on previously seen environments while still being able to adapt to new environments. These methods enable continual training of speech emotion recognition models following deployment while retaining previous knowledge, working towards a more general, adaptable, acoustic model.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/perez22_interspeech.pdf"><papertitle>Mind the gap: On the value of silence representations to lexical-based speech emotion recognition</papertitle></a>
              <br>
              <strong>Matthew Perez</strong>,
              Mimansa Jaiswal,
              Minxue Niu,
              Cristina Gorrostieta,
              Matthew Roddy,
              Kye Taylor, 
              Reza Lotfian,
              John Kane,
              Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2022
              <br>
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/perez22_interspeech.pdf">Paper</a>
              <p></p>
              <p> Utilizing non-speech frames (i.e. silence) in a BERT-framework to improve speech emotion recognition. We find that silence has as significant impact on predicting valence and our token analysis suggests that the presence of and proximity to silence are important factors in latent text features extracted from BERT.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/romana22_interspeech.pdf"><papertitle>Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech</papertitle></a>
              <br>
              Amrit Romana, 
              Minxue Niu,
              <strong>Matthew Perez</strong>,
              Angela Roberts, 
              Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2022
              <br>
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/romana22_interspeech.pdf">Paper</a>
              <p></p>
              <p> This work investigates the use of BERT for dysfluency detection and categorization. We propose finetuning BERT with an additional triplet loss function in order to specifically focus on reptitions and revisions (which are categories which underperform using a baseline BERT model). We show that the added triplet loss leads to improved BERT performance for both revisions and repetitions while preserving performance on other categories.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2109.13815.pdf"><papertitle>Articulatory Coordination for Speech Motor Tracking in Huntington Disease</papertitle></a>
              <br>
							<strong>Matthew Perez</strong>,
							Amrit Romana, 
              Angela Roberts,
							Noelle Carlozzi,
							Jennifer Ann Miner,
							Praveen Dayalu, 
							Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2109.13815.pdf">Paper</a> |
              <a href="https://github.com/matthewkperez/VTC-features">Code</a>
              <p></p>
              <p> Acoustic biomarkers which capture articulatory coordination are particularly promising for characterizing motor symptom progression in people affected by Huntington Disease. In this paper, we utilize Vocal Tract Coordination (VTC) features extracted from read speech to estimate a motor severity score and show these features outperform other common baselines.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/Romana2021_Interspeech.pdf"><papertitle>Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinsonâ€™s Disease</papertitle></a>
              <br>
              Amrit Romana, 
              John Bandon,
              <strong>Matthew Perez</strong>,
              Stephanie Gutierrez,
              Richard Richter,
              Angela Roberts, 
              Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/Romana2021_Interspeech.pdf">Paper</a> | <a href="https://github.com/amritkromana/reading-error-disfluency-features">Code</a>
              <p></p>
              <p> This work investigates the use of speech errors and disfluencies in people with Parkinson's Disease as a means of analyzing cognitive impairment. In this study, we focus on read speech, which offers a controlled template from which we can detect errors and disfluencies, and we analyze how errors and disfluencies vary with cognitive impairment</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/2021.naacl-main.377.pdf"><papertitle>Learning Paralinguistic Attributes from Audiobooks with Voice Conversion</papertitle></a>
              <br>
              Zakaria Aldeneh, 
              <strong>Matthew Perez</strong>,
              Emily Mower Provost
              <br>
              <em>NAACL</em>, 2021 
              <br>
              <a href="https://aclanthology.org/2021.naacl-main.377.pdf">Paper</a>
              <p></p>
              <p> Paralinguistic tasks, specifically speech emotion recognition, have limited access to large datasets with accurate labels, which makes it difficult to train models that capture paralinguistic attributes via supervised learning. In this work, we propose the Expressive Voice Conversion Autoencoder (EVoCA), which is a framework for capturing paralinguistic (e.g., emotion) attributes from a large-scale (i.e., 200 hours) audio-textual data without requiring manual emotion annotations. The proposed network utilizes the conversion of synthesized (neutral) speech and real (expressive) speech in order to learn what makes speech expressive in an unsupervised manner. </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/2020_Interspeech_Perez.pdf"><papertitle>Aphasic Speech Recognition using a Mixture of Speech Intelligibility Experts</papertitle></a>
              <br>
              <strong>Matthew Perez</strong>,
              Zakaria Aldeneh, 
              Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2020
              <br>
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/2020_Interspeech_Perez.pdf">Paper</a>
              <p></p>
              <p> Automatic speech recognition (ASR) is a key component for automatic, aphasic speech analysis. However, current approaches of using a standard, one-size-fits-all ASR model might be sub-optimal due to the wide range of speech intelligibility that exists both within and between speakers. This work investigates how speech intelligibility can be estimated using a neural network  and how intelligibility variability can be addressed within an acoustic model architecture using a mixture of experts. Our results show that this style of modeling leads to significant phone recognition improvement compared to a traditional, one-size-fits-all model.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7685291/"><papertitle>Classification of Huntington Disease using Acoustic and Lexical Features</papertitle></a>
              <br>
              <strong>Matthew Perez</strong>,
              Wenyu Jin, 
              Duc Le, 
              Noelle Carlozzi,
              Praveen Dayalu, 
              Angela Roberts,
              Emily Mower Provost
              <br>
              <em>INTERSPEECH</em>, 2018
              <br>
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7685291/">Paper</a>
              <p></p>
              <p> This works presents a pipeline for an automatic, end-to-end classification system using speech as the primary input for predicting Huntington Disease. We explore using transcript-based features to capture speech-characteristics of interest and use methods such as k-Nearest Neighbors (with euclidean and dynamic time warped distances) as well as more modern neural net approaches for classification. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/7762779"><papertitle>Portable mTBI Assessment Using Temporal and Frequency Analysis of Speech</papertitle></a>
              <br>
              Louis Daudet, 
              Nikhil Yadav, 
              <strong>Matthew Perez</strong>,
              Christian Poellabauer,
              Sandra Schneider, 
              Alan Huebner
              <br>
              <em>IEEE Journal of Biomedical and Health Informatics </em>, 2016
              <br>
              <a href="https://ieeexplore.ieee.org/document/7762779">Paper</a>
              <p></p>
              <p> This work investigates the use of mobile devices for the extraction and analysis of various acoustic features at detecting mild traumatic brain injury (mTBI). Our results suggest strong correlation between certain temporal and frequency features and likelihood of a concussion. 
              </p>
            </td>
          </tr> 

					
					
        </tbody></table>
      </td>
    </tr>
  </table>
  <script>
    var lastModified = document.lastModified;     // Get the full lastModified string
    var datePart = lastModified.split(" ")[0];    // Split by space and take the first part (date)
    document.getElementById('lastModified').textContent = 'Webpage Last Updated: ' + datePart;
  </script>
</body>

<a href="https://jonbarron.info/">Website Template</a>

</html>
