<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Matthew Perez</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Matthew Perez</name>
              </p>
              <p>
              My name is Matthew Perez and I am a Computer Science Ph.D. candidate at the University of Michigan. I work with Dr. Emily Mower Provost at the <a href="https://web.eecs.umich.edu/~emilykmp/chai/">Computational Human Artificial Intelligence (CHAI) Lab</a>. My interest is broadly in using speech analysis and machine learning to create intelligent systems that understand principles relating to human health and behavior. Currently, my focus is on applying deep learning techniques to improve speech recognition for low resource applications (i.e. disordered speech).
              </p>
              <p>
                I received my M.S. in Computer Science and Engineering at the University of Michigan and my B.S. in Computer Science at the University of Notre Dame.  I have  been fortunate to receive the GEM Fellowship ('19) and the NSF Graduate Research Fellowship ('20).
              </p>

              <p style="text-align:center">
                <a href="mailto:mkperez@umich.edu">Email</a> &nbsp/&nbsp
                <a href="data/Perez_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HRLbzeYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/matthewkperez/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/matt.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/matt.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



					
					
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hypernerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/hypernerf_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function hypernerf_start() {
                  document.getElementById('hypernerf_image').style.opacity = "1";
                }

                function hypernerf_stop() {
                  document.getElementById('hypernerf_image').style.opacity = "0";
                }
                hypernerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Articulatory Coordination for Speech Motor Tracking in Huntington Disease</papertitle></a>
              <br>
							<strong>Matthew Perez</strong>,
							<a>Amrit Romana</a>, 
							<a>Peter Hedman</a>,
              <a>Angela Roberts</a>,
							<a>Noelle Carlozzi</a>,
							<a>Jennifer Ann Miner</a>,
							<a>Praveen Dayalu</a>, 
							<a>Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2109.13815.pdf">Paper</a>
              <a href="https://github.com/matthewkperez/VTC-features">Github</a>
              <p></p>
              <p> In this paper, we present an experiment that uses Vocal Tract Coordination (VTC) features extracted from read speech to estimate a motor score</p>
            </td>
          </tr> 


					
					
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
