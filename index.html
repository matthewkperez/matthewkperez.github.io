<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Matthew Perez</title>
  
  <meta name="author" content="Matthew Perez">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Matthew Perez</name>
              </p>
              <p>
              My name is Matthew Perez and I am a Computer Science Ph.D. candidate at the University of Michigan. I work with Dr. Emily Mower Provost at the <a href="https://web.eecs.umich.edu/~emilykmp/chai/">Computational Human Artificial Intelligence (CHAI) Lab</a>. My interest is broadly in using speech analysis and machine learning to create intelligent systems that understand principles relating to human health and behavior. Currently, my focus is on applying deep learning techniques to improve speech recognition for low resource applications (i.e. disordered speech).
              </p>
              <p>
                I received my M.S. in Computer Science and Engineering at the University of Michigan and my B.S. in Computer Science at the University of Notre Dame.  I have  been fortunate to receive the GEM Fellowship ('19) and the NSF Graduate Research Fellowship ('20).
              </p>

              <p style="text-align:center">
                <a href="mailto:mkperez@umich.edu">Email</a> &nbsp/&nbsp
                <a href="data/Perez_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HRLbzeYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/matthewkperez/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/matt.jpeg">
                <!-- <img style="width:100%;max-width:100%" alt="profile photo" src="images/matt.jpeg" class="hoverZoomLink"> -->
                <!-- <img height="150" width="200" alt="profile photo" src="images/matt.jpeg"> -->
                <img height="150" width="200" alt="profile photo" src="images/casual.jpeg">
              </a>
            </td>
          </tr>
        </tbody></table>



				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Mind the gap: On the value of silence representations to lexical-based speech emotion recognition</papertitle>
              <br>
              <strong>Matthew Perez</strong>,
              <a href="https://mimansajaiswal.github.io/">Mimansa Jaiswal</a>, 
              Minxue Niu,
              Cristina Gorrostieta,
              Matthew Roddy,
              Kye Taylor, 
              Reza Lotfian,
              John Kane,
              <a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2022
              <br>
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/perez22_interspeech.pdf">Paper</a> |
              <p></p>
              <p> Utilizing non-speech frames (i.e. silence) in a BERT-framework to improve speech emotion recognition. We find that silence has as significant impact on predicting valence and our token analysis suggests that the presence of and proximity to silence are important factors in latent text features extracted from BERT.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech</papertitle>
              <br>
              <a href="http://www-personal.umich.edu/~aromana/">Amrit Romana</a>, 
              Minxue Niu,
              <strong>Matthew Perez</strong>,
              Angela Roberts, 
              <a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2022
              <br>
              <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/romana22_interspeech.pdf">Paper</a>
              <p></p>
              <p> This work investigates the use of BERT for dysfluency detection and categorization. We propose finetuning BERT with an additional triplet loss function in order to specifically focus on reptitions and revisions (which are categories which underperform using a baseline BERT model). We show that the added triplet loss leads to improved BERT performance for both revisions and repetitions while preserving performance on other categories.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Articulatory Coordination for Speech Motor Tracking in Huntington Disease</papertitle>
              <br>
							<strong>Matthew Perez</strong>,
							<a href="http://www-personal.umich.edu/~aromana/">Amrit Romana</a>, 
              Angela Roberts,
							Noelle Carlozzi,
							Jennifer Ann Miner,
							Praveen Dayalu, 
							<a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2109.13815.pdf">Paper</a> |
              <a href="https://github.com/matthewkperez/VTC-features">Github</a>
              <p></p>
              <p> Acoustic biomarkers which capture articulatory coordination are particularly promising for characterizing motor symptom progression in people affected by Huntington Disease. In this paper, we utilize Vocal Tract Coordination (VTC) features extracted from read speech to estimate a motor severity score and show these features outperform other common baselines.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinsonâ€™s Disease</papertitle>
              <br>
              <a href="http://www-personal.umich.edu/~aromana/">Amrit Romana</a>, 
              John Bandon,
              <strong>Matthew Perez</strong>,
              Stephanie Gutierrez,
              Richard Richter,
              Angela Roberts, 
              <a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/Romana2021_Interspeech.pdf">Paper</a>
              <p></p>
              <p> This work investigates the use of speech errors and disfluencies in people with Parkinson's Disease as a means of analyzing cognitive impairment. In this study, we focus on read speech, which offers a controlled template from which we can detect errors and disfluencies, and we analyze how errors and disfluencies vary with cognitive impairment</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning Paralinguistic Attributes from Audiobooks with Voice Conversion</papertitle>
              <br>
              <a href="http://www-personal.umich.edu/~aldeneh/">Zakaria Aldeneh</a>, 
              <strong>Matthew Perez</strong>,
              <a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>NAACL</em>, 2021 
              <br>
              <a href="https://aclanthology.org/2021.naacl-main.377.pdf">Paper</a>
              <p></p>
              <p> Paralinguistic tasks, specifically speech emotion recognition, have limited access to large datasets with accurate labels, which makes it difficult to train models that capture paralinguistic attributes via supervised learning. In this work, we propose the Expressive Voice Conversion Autoencoder (EVoCA), which is a framework for capturing paralinguistic (e.g., emotion) attributes from a large-scale (i.e., 200 hours) audio-textual data without requiring manual emotion annotations. The proposed network utilizes the conversion of synthesized (neutral) speech and real (expressive) speech in order to learn what makes speech expressive in an unsupervised manner. </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Aphasic Speech Recognition using a Mixture of Speech Intelligibility Experts</papertitle>
              <br>
              <strong>Matthew Perez</strong>,
              <a href="http://www-personal.umich.edu/~aldeneh/">Zakaria Aldeneh</a>, 
              <a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2020
              <br>
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/2020_Interspeech_Perez.pdf">Paper</a>
              <p></p>
              <p> Automatic speech recognition (ASR) is a key component for automatic, aphasic speech analysis. However, current approaches of using a standard, one-size-fits-all ASR model might be sub-optimal due to the wide range of speech intelligibility that exists both within and between speakers. This work investigates how speech intelligibility can be estimated using a neural network  and how intelligibility variability can be addressed within an acoustic model architecture using a mixture of experts. Our results show that this style of modeling leads to significant phone recognition improvement compared to a traditional, one-size-fits-all model.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Classification of Huntington Disease using Acoustic and Lexical Features</papertitle>
              <br>
              <strong>Matthew Perez</strong>,
              Wenyu Jin, 
              <a href="http://websites.umich.edu/~ducle/">Duc Le</a>, 
              Noelle Carlozzi,
              Praveen Dayalu, 
              Angela Roberts,
              <a href="https://web.eecs.umich.edu/~emilykmp/">Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2018
              <br>
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7685291/">Paper</a>
              <p></p>
              <p> This works presents a pipeline for an automatic, end-to-end classification system using speech as the primary input for predicting Huntington Disease. We explore using transcript-based features to capture speech-characteristics of interest and use methods such as k-Nearest Neighbors (with euclidean and dynamic time warped distances) as well as more modern neural net approaches for classification. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Portable mTBI Assessment Using Temporal and Frequency Analysis of Speech</papertitle>
              <br>
              Louis Daudet, 
              <a href="https://www.stjohns.edu/academics/faculty/nikhil-yadav-phd">Nikhil Yadav</a>, 
              <strong>Matthew Perez</strong>,
              <a href="https://www3.nd.edu/~cpoellab/">Christian Poellabauer</a>,
              Sandra Schneider, 
              <a href="https://acms.nd.edu/people/alan-huebner/">Alan Huebner</a>
              <br>
              <em>IEEE Journal of Biomedical and Health Informatics </em>, 2016
              <br>
              <a href="https://ieeexplore.ieee.org/document/7762779">Paper</a>
              <p></p>
              <p> This work investigates the use of mobile devices for the extraction and analysis of various acoustic features at detecting mild traumatic brain injury (mTBI). Our results suggest strong correlation between certain temporal and frequency features and likelihood of a concussion. 
              </p>
            </td>
          </tr> 

					
					
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

<a href="https://jonbarron.info/">Website Template</a>

</html>
