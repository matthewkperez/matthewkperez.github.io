<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Matthew Perez</title>
  
  <meta name="author" content="Matthew Perez">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Matthew Perez</name>
              </p>
              <p>
              My name is Matthew Perez and I am a Computer Science Ph.D. candidate at the University of Michigan. I work with Dr. Emily Mower Provost at the <a href="https://web.eecs.umich.edu/~emilykmp/chai/">Computational Human Artificial Intelligence (CHAI) Lab</a>. My interest is broadly in using speech analysis and machine learning to create intelligent systems that understand principles relating to human health and behavior. Currently, my focus is on applying deep learning techniques to improve speech recognition for low resource applications (i.e. disordered speech).
              </p>
              <p>
                I received my M.S. in Computer Science and Engineering at the University of Michigan and my B.S. in Computer Science at the University of Notre Dame.  I have  been fortunate to receive the GEM Fellowship ('19) and the NSF Graduate Research Fellowship ('20).
              </p>

              <p style="text-align:center">
                <a href="mailto:mkperez@umich.edu">Email</a> &nbsp/&nbsp
                <a href="data/Perez_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HRLbzeYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/matthewkperez/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/matt.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/matt.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Articulatory Coordination for Speech Motor Tracking in Huntington Disease</papertitle></a>
              <br>
							<strong>Matthew Perez</strong>,
							<a>Amrit Romana</a>, 
							<a>Peter Hedman</a>,
              <a>Angela Roberts</a>,
							<a>Noelle Carlozzi</a>,
							<a>Jennifer Ann Miner</a>,
							<a>Praveen Dayalu</a>, 
							<a>Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2109.13815.pdf">Paper</a> |
              <a href="https://github.com/matthewkperez/VTC-features">Github</a>
              <p></p>
              <p> In this paper, we present an experiment that uses Vocal Tract Coordination (VTC) features extracted from read speech to estimate a motor score</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinsonâ€™s Disease</papertitle></a>
              <br>
              <a>Amrit Romana</a>, 
              <a>John Bandon</a>,
              <strong>Matthew Perez</strong>,
              <a>Stephanie Gutierrez</a>,
              <a>Richard Richter</a>,
              <a>Angela Roberts</a>, 
              <a>Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2021 
              <br>
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/Romana2021_Interspeech.pdf">Paper</a>
              <p></p>
              <p> This work investigates the use of speech errors and disfluencies in people with Parkinson's Disease as a means of analyzing cognitive impairment. In this study, we focus on read speech, which offers a controlled template from which we can detect errors and disfluencies, and we analyze how errors and disfluencies vary with cognitive impairment</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Learning Paralinguistic Attributes from Audiobooks with Voice Conversion</papertitle></a>
              <br>
              <a>Zakaria Aldeneh</a>, 
              <strong>Matthew Perez</strong>,
              <a>Emily Mower Provost</a>
              <br>
              <em>NAACL</em>, 2021 
              <br>
              <a href="https://aclanthology.org/2021.naacl-main.377.pdf">Paper</a>
              <p></p>
              <p> Paralinguistic tasks, specifically speech emotion recognition, have limited access to large datasets with accurate labels, which makes it difficult to train models that capture paralinguistic attributes via supervised learning. In this work, we propose the Expressive Voice Conversion Autoencoder (EVoCA), which is a framework for capturing paralinguistic (e.g., emotion) attributes from a large-scale (i.e., 200 hours) audio-textual data without requiring manual emotion annotations. The proposed network utilizes the conversion of synthesized (neutral) speech and real (expressive) speech in order to learn what makes speech expressive in an unsupervised manner. </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Aphasic Speech Recognition using a Mixture of Speech Intelligibility Experts</papertitle></a>
              <br>
              <strong>Matthew Perez</strong>,
              <a>Zakaria Aldeneh</a>, 
              <a>Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2020
              <br>
              <a href="https://web.eecs.umich.edu/~emilykmp/EmilyPapers/2020_Interspeech_Perez.pdf">Paper</a>
              <p></p>
              <p> Automatic speech recognition (ASR) is a key component for automatic, aphasic speech analysis. However, current approaches of using a standard, one-size-fits-all ASR model might be sub-optimal due to the wide range of speech intelligibility that exists both within and between speakers. This work investigates how speech intelligibility can be estimated using a neural network  and how intelligibility variability can be addressed within an acoustic model architecture using a mixture of experts. Our results show that this style of modeling leads to significant phone recognition improvement compared to a traditional, one-size-fits-all model.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Aphasic Speech Recognition using a Mixture of Speech Intelligibility Experts</papertitle></a>
              <br>
              <strong>Matthew Perez</strong>,
              <a>Wenyu Jin</a>, 
              <a>Duc Le</a>, 
              <a>Noelle Carlozzi</a>,
              <a>Praveen Dayalu</a>, 
              <a>Angela Roberts</a>,
              <a>Emily Mower Provost</a>
              <br>
              <em>INTERSPEECH</em>, 2018
              <br>
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7685291/">Paper</a>
              <p></p>
              <p> This works presents a pipeline for an automatic, end-to-end classification system using speech as the primary input for predicting Huntington Disease. We explore using transcript-based features to capture speech-characteristics of interest and use methods such as k-Nearest Neighbors (with euclidean and dynamic time warped distances) as well as more modern neural net approaches for classification. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a><papertitle>Portable mTBI Assessment Using Temporal and Frequency Analysis of Speech</papertitle></a>
              <br>
              <a>Louis Daudet</a>, 
              <a>Nikhil Yadav</a>, 
              <strong>Matthew Perez</strong>,
              <a>Christian Poellabauer</a>,
              <a>Sandra Schneider</a>, 
              <a>Alan Huebner</a>
              <br>
              <em>IEEE Journal of Biomedical and Health Informatics </em>, 2016
              <br>
              <a href="https://ieeexplore.ieee.org/document/7762779">Paper</a>
              <p></p>
              <p> This work investigates the use of mobile devices for the extraction and analysis of various acoustic features at detecting mild traumatic brain injury (mTBI). Our results suggest strong correlation between certain temporal and frequency features and likelihood of a concussion. 
              </p>
            </td>
          </tr> 

					
					
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
